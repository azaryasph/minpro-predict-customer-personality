{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Customer Personality to Boost Marketing Campaign by Using Machine Learning\n",
    "\n",
    "- Name : Azarya Yehezkiel Pinondang Sipahutar\n",
    "\n",
    "**Project Goals**<br> Segment our customer base using clustering techniques. This segmentation will enable our marketing team to tailor their strategies to the specific needs, behaviors, and preferences of each customer group. By doing so, we hope to increase customer engagement, boost revenue, and reduce marketing costs by focusing our efforts where they are most likely to have an impact.<br><br>\n",
    "**Objective** <br>The objective of this project is to develop a KMeans Clustering Model. This model will analyze our customer data and identify distinct clusters based on various customer attributes. The resulting clusters will provide a clearer understanding of our diverse customer base, allowing us to target our marketing efforts more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Conversion Rate Analysis Based On Income, Spending And Age\n",
    "Goals : Find a pattern of Customer behavior.<br><br>\n",
    "Objective : \n",
    "- Feature engineering \n",
    "- Exploratory Data Analysis (EDA)  \n",
    "- Analyze Conversion Rate with other variables such as age, income, expenses, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data & Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('./data/marketing_campaign_data.csv')\n",
    "display(df.sample(4))\n",
    "\n",
    "# Display the information about the DataFrame\n",
    "print(\"DataFrame Information:\")\n",
    "df.info()\n",
    "\n",
    "# Create a DataFrame for the description\n",
    "desc_df = df.describe().transpose()\n",
    "\n",
    "# Add the number of unique values to the description DataFrame\n",
    "desc_df['unique'] = df.nunique()\n",
    "\n",
    "# Display the description DataFrame\n",
    "print(\"\\nDataFrame Description:\")\n",
    "display(desc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initial inspection of the dataset, we have identified several key points that will influence our data preprocessing steps:\n",
    "\n",
    "1. **Missing Values:** The dataset contains missing values that need to be handled. Depending on the nature and amount of missing data, we may choose to fill these with appropriate values or drop the rows/columns with missing data.\n",
    "\n",
    "2. **Outliers:** Some features in the dataset exhibit outliers. These can significantly skew our statistical analysis and machine learning model performance. We will need to identify these outliers and decide on the best strategy to handle them, such as capping, transforming, or removing them.\n",
    "\n",
    "3. **Redundant Index Column:** The `Unnamed: 0` feature appears to be an index column. Since Pandas DataFrames automatically provide an index, this column is redundant and will be dropped during preprocessing.\n",
    "\n",
    "4. **Non-informative Columns:** The `Z_CostContact` and `Z_Revenue` features only contain a single unique value. These features do not provide any variability or valuable information for our analysis or predictive modeling, and will therefore be dropped during preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "In this section, we create new features to better understand our customers and their behaviors. Here's a brief explanation of each new feature:\n",
    "\n",
    "1. **Age**: This feature represents the age of each customer. It is calculated by subtracting the `Year_Birth` feature from the current year.\n",
    "\n",
    "2. **AgeGroup**: This feature categorizes customers into different age groups for easier analysis. The age groups are determined based on the customer's `Age` range, as suggested by this [article](https://www.researchgate.net/figure/Age-intervals-and-age-groups_tbl1_228404297). The minimum age in this dataset is 28.\n",
    "\n",
    "3. **Parent**: This feature indicates the parental status of each customer. It is created based on whether a customer has a kid at home or not.\n",
    "\n",
    "4. **NumChild**: This feature represents the total number of children each customer has. It is calculated from the sum of the `KidHome` and `TeenHome` features.\n",
    "\n",
    "5. **TotalAcceptedCmp**: This feature represents the total number of campaigns each customer accepted after the campaign was carried out. It is calculated from the sum of the `AcceptedCmp1` to `AcceptedCmp5` features.\n",
    "\n",
    "6. **TotalSpending** : This feature represents the total spending each customer spended on our platform. It is calculated from the sum of `MntCoke`, `MntFruits`, `MntMeatProducts`, `MntFishProducts`, `MntSweetProducts`, and `MntGoldProds` features.\n",
    "\n",
    "7. **Total Trx**: This feature represents the total number of transactions the customer made in our store, either offline or online. It is calculated from the `NumDealsPurchases`, `NumWebPurchases`, `NumCatalogPurchases`, and `NumStorePurchases` features.\n",
    "\n",
    "9. **ConversionRate**: This feature represents the percentage of website visitors who complete a web purchase. It is a key metric for understanding the effectiveness of our *online sales efforts*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe to avoid modifying the original data\n",
    "dfe = df.copy()\n",
    "\n",
    "# Calculate the age of each customer based on their year of birth\n",
    "dfe['Age'] = 2024 - dfe['Year_Birth']\n",
    "\n",
    "# Categorize customers into age groups based on their age\n",
    "age_grouping = [\n",
    "    (dfe['Age'] >= 60),\n",
    "    (dfe['Age'] >= 40 ) & (dfe['Age'] < 60),\n",
    "    (dfe['Age'] >= 28) & (dfe['Age'] < 40)\n",
    "]\n",
    "age_category = ['Old Adults', 'Middled-aged Adults', 'Young Adults']\n",
    "dfe['AgeGroup'] = np.select(age_grouping, age_category)\n",
    "\n",
    "# Determine whether each customer has a kid at home\n",
    "def has_kid(row):\n",
    "    if row['Kidhome'] > 0 or row['Teenhome'] > 0:\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'\n",
    "dfe['Parent'] = dfe.apply(has_kid, axis=1)\n",
    "\n",
    "# Calculate the total number of children each customer has\n",
    "dfe['NumChild'] = dfe['Kidhome'] + dfe['Teenhome']\n",
    "\n",
    "# Calculate the total number of campaigns each customer accepted\n",
    "dfe['TotalAcceptedCmp'] = dfe['AcceptedCmp1'] + dfe['AcceptedCmp2'] + dfe['AcceptedCmp3'] + dfe['AcceptedCmp4'] + dfe['AcceptedCmp5']\n",
    "\n",
    "# Calculate the total spending of each customer across all product categories\n",
    "dfe['TotalSpending'] = dfe['MntCoke'] + dfe['MntFruits'] + dfe['MntMeatProducts'] + dfe['MntFishProducts'] + dfe['MntSweetProducts'] + dfe['MntGoldProds']\n",
    "\n",
    "# Calculate the total number of transactions each customer made\n",
    "dfe['TotalTrx'] = dfe['NumDealsPurchases'] + dfe['NumWebPurchases'] + dfe['NumCatalogPurchases'] + dfe['NumStorePurchases']\n",
    "\n",
    "# Calculate the conversion rate for each customer (the number of web purchases divided by the number of web visits)\n",
    "dfe['ConversionRate'] =  dfe['NumWebPurchases'] / dfe['NumWebVisitsMonth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outlier Checking (Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric features for exploratory data analysis\n",
    "numeric_features = [\n",
    "    'Income', 'Recency', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', \n",
    "    'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', \n",
    "    'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Age', \n",
    "    'TotalSpending', 'TotalTrx', 'ConversionRate'\n",
    "]\n",
    "\n",
    "# Create a subplot grid for boxplots\n",
    "fig, axes = plt.subplots(2, 8, figsize=(24, 8))\n",
    "\n",
    "# Set the title and background color for the figure\n",
    "fig.suptitle('Outlier Checking of Necessary Numeric Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "fig.set_facecolor('#E8E8E8')\n",
    "\n",
    "# Loop over each numeric feature and create a boxplot on a separate subplot\n",
    "for feature, ax in zip(numeric_features, axes.flatten()):\n",
    "    # Create a boxplot for the current feature\n",
    "    sns.boxplot(y=dfe[feature], ax=ax, color='#D1106F', linewidth=2.1, width=0.55, fliersize=3.5)\n",
    "    \n",
    "    # Set the title for the current subplot\n",
    "    ax.set_title(f'Boxplot of {feature}', fontsize=14, fontweight='bold', pad=5)\n",
    "    \n",
    "    # Remove gridlines from the current subplot\n",
    "    ax.grid(False)\n",
    "\n",
    "# Adjust the layout to prevent overlapping of subplots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, we have identified outliers in the following features:\n",
    "\n",
    "1. `Income`\n",
    "2. `MntMeatProducts`\n",
    "3. `MntSweetProducts`\n",
    "4. `MntGoldProds`\n",
    "5. `NumDealsPurchases`\n",
    "6. `NumWebPurchases`\n",
    "7. `NumCatalogPurchases`\n",
    "8. `NumWebVisitsMonth`\n",
    "9. `Age`\n",
    "10. `TotalTrx`\n",
    "11. `ConversionRate`\n",
    "\n",
    "- Outliers can significantly skew the results of our data analysis and predictive modeling process. They can be caused by various factors such as measurement errors, data entry errors, or extreme variation in the data.\n",
    "\n",
    "- In this case, we have decided to cap the outliers to the lower/upper bound. This approach involves replacing the extreme values with a specified minimum and maximum value. It is a suitable method when we don't want to lose data, but at the same time, we want to limit the effect of the extreme values.\n",
    "\n",
    "- This method is particularly beneficial for our unsupervised machine learning model, as it can help to improve the performance of the model by reducing the impact of the outliers on the model's learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Distribution (Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric features for exploratory data analysis\n",
    "numeric_features = [\n",
    "    'Income', 'Recency', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', \n",
    "    'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', \n",
    "    'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Age', \n",
    "    'TotalSpending', 'TotalTrx', 'ConversionRate'\n",
    "]\n",
    "\n",
    "# Create a subplot grid for KDE plots\n",
    "fig, axes = plt.subplots(2, 8, figsize=(28, 9))\n",
    "\n",
    "# Set the title and background color for the figure\n",
    "fig.suptitle('KDE plot for Necessary Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "fig.set_facecolor('#E8E8E8')\n",
    "\n",
    "# Loop over each numeric feature and create a KDE plot on a separate subplot\n",
    "for feature, ax in zip(numeric_features, axes.flatten()):\n",
    "    # Create a KDE plot for the current feature\n",
    "    sns.kdeplot(x=dfe[feature], ax=ax, color='#D1106F', linewidth=0.7, fill=True)\n",
    "    \n",
    "    # Set the title for the current subplot\n",
    "    ax.set_title(f'Distribution of {feature}', fontsize=14, fontweight='bold', pad=5)\n",
    "    \n",
    "    # Remove gridlines from the current subplot\n",
    "    ax.grid(False)\n",
    "\n",
    "# Adjust the layout to prevent overlapping of subplots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my analysis of the kernel density estimation (KDE) plot for this dataset, i observed the following:\n",
    "\n",
    "- The distribution of most features in this dataset is positively skewed. This indicates that the majority of the values lie to the right of the mean, with a tail extending towards the left.\n",
    "\n",
    "- In case of missing values in the data, my strategy is to fill these gaps with the median value of the respective feature. The rationale behind this choice is that the median is robust to outliers, meaning it provides a better central tendency estimate for skewed distributions compared to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Distribution (Categoric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subplot grid for pie charts\n",
    "fig, axs = plt.subplots(1, 2, figsize=(24, 12))\n",
    "fig.set_facecolor('#E8E8E8')\n",
    "\n",
    "# Define color palette\n",
    "palette = ['#00D19B', '#D1106F', '#25A9D9']\n",
    "\n",
    "# Get counts of each age group\n",
    "age_counts = dfe['AgeGroup'].value_counts()\n",
    "\n",
    "# Create pie chart for age group distribution\n",
    "patches, texts, autotexts = axs[0].pie(\n",
    "    age_counts, \n",
    "    colors=palette, \n",
    "    autopct='%1.1f%%', \n",
    "    textprops={'size': 20}\n",
    ")\n",
    "\n",
    "# Add legend and title for the first pie chart\n",
    "axs[0].legend(patches, age_counts.index, loc=\"best\", fontsize='x-large')\n",
    "axs[0].set_title(\n",
    "    \"Distribution of Customer by Age Group\", \n",
    "    fontsize=22, \n",
    "    fontweight='bold', \n",
    "    y=1.02\n",
    ")\n",
    "\n",
    "# Get counts of each parent group\n",
    "parent_counts = dfe['Parent'].value_counts()\n",
    "\n",
    "# Update color palette for the second pie chart\n",
    "palette = ['#00D19B', '#D1106F']\n",
    "\n",
    "# Create pie chart for parent group distribution\n",
    "patches, texts, autotexts = axs[1].pie(\n",
    "    parent_counts, \n",
    "    colors=palette, \n",
    "    autopct='%1.1f%%', \n",
    "    textprops={'size': 20}\n",
    ")\n",
    "\n",
    "# Add legend and title for the second pie chart\n",
    "axs[1].legend(patches, parent_counts.index, loc=\"best\", fontsize='x-large')\n",
    "axs[1].set_title(\n",
    "    \"Parent Customer Distribution\", \n",
    "    fontsize=22, \n",
    "    fontweight='bold', \n",
    "    y=1.02\n",
    ")\n",
    "\n",
    "# Adjust the layout to prevent overlapping of subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subplot grid for count plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), facecolor='#E8E8E8')\n",
    "\n",
    "# Define a function to annotate count plots\n",
    "def annotate_countplot(countplot):\n",
    "    for p in countplot.patches:\n",
    "        height = p.get_height()\n",
    "        countplot.text(\n",
    "            p.get_x() + p.get_width() / 2.,\n",
    "            height + 10,\n",
    "            '{:1.0f}'.format(height),\n",
    "            ha=\"center\",\n",
    "            fontweight='bold'\n",
    "        )\n",
    "\n",
    "# Define color palette and order of education levels\n",
    "palette = ['#D1106F', '#00D19B', '#25A9D9', '#D16F11', '#6F11D1']\n",
    "edu_order = ['SMA', 'D3', 'S1', 'S2', 'S3']\n",
    "\n",
    "# Create count plot for education level\n",
    "countplot = sns.countplot(\n",
    "    data=dfe, \n",
    "    x='Education', \n",
    "    hue='Education', \n",
    "    order=edu_order, \n",
    "    palette=palette, \n",
    "    ax=axs[0], \n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Annotate the count plot\n",
    "annotate_countplot(countplot)\n",
    "\n",
    "# Set the title, labels, and grid for the first subplot\n",
    "axs[0].set_ylim(0, 1250)\n",
    "axs[0].set_title(\"Customer Distribution by Education Level\", fontsize=18, fontweight='bold', y=1.03)\n",
    "axs[0].set_xlabel('Education Level', fontsize=12)\n",
    "axs[0].set_ylabel('Count', fontsize=12)\n",
    "axs[0].grid(False)\n",
    "\n",
    "# Update color palette for the second count plot\n",
    "palette = ['#D1106F', '#00D19B', '#25A9D9', '#D16F11', '#6F11D1', '#11D1D1']\n",
    "\n",
    "# Create count plot for marital status\n",
    "countplot = sns.countplot(\n",
    "    data=dfe, \n",
    "    x='Marital_Status', \n",
    "    hue='Marital_Status', \n",
    "    palette=palette, \n",
    "    ax=axs[1], \n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Annotate the count plot\n",
    "annotate_countplot(countplot)\n",
    "\n",
    "# Set the title, labels, and grid for the second subplot\n",
    "axs[1].set_ylim(0, 950)\n",
    "axs[1].set_title(\"Customer Distribution by Marital Status\", fontsize=18, fontweight='bold', y=1.03)\n",
    "axs[1].set_xlabel('Marital Status', fontsize=12)\n",
    "axs[1].set_ylabel('Count', fontsize=12)\n",
    "axs[1].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our analysis of the customer data, we can draw several key insights about the demographic profile of our majority customer base:\n",
    "\n",
    "- Age Group: The majority of our customers fall within the middle-aged group, specifically between 40 and 59 years old. This could indicate that our products or services are particularly appealing to individuals in this age range.\n",
    "\n",
    "- Parental Status: A significant proportion of our customers have children at home. This suggests that our offerings may cater well to the needs of parents or families.\n",
    "\n",
    "- Education Level: Most of our customers have achieved a level of education up to a Bachelor's Degree. This could reflect the affordability, accessibility, or appeal of our products or services to individuals with this level of education.\n",
    "\n",
    "- Marital Status: The majority of our customers are married. This might indicate that our products or services are popular among couples or that they cater to the needs of married individuals.\n",
    "\n",
    "These insights can help us better understand our customer base and tailor our marketing strategies, product development, and services to meet their needs and preferences. However, it's important to remember that these are general trends and there may be significant variation within these groups. Further segmentation and analysis could provide more nuanced insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subplot grid for scatter plots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(24, 16))\n",
    "fig.set_facecolor('#E8E8E8')\n",
    "\n",
    "# Plot 1: Customer Conversion Rate and Income Correlation\n",
    "sns.scatterplot(x='Income', y='ConversionRate', data=dfe, color='#D1106F', ax=axs[0, 0])\n",
    "axs[0, 0].set_xlim(0, 200000000)\n",
    "axs[0, 0].set_ylim(0, 4.7)\n",
    "axs[0, 0].axvline(x=110000000, color='b', linestyle='--')\n",
    "axs[0, 0].set_title(\"Customer Conversion Rate and Income Correlation\", fontsize=19, fontweight='bold', y=1.02)\n",
    "axs[0, 0].set_xlabel('Income', fontsize=13.5)\n",
    "axs[0, 0].set_ylabel('Conversion Rate', fontsize=13.5)\n",
    "axs[0, 0].grid(False)\n",
    "\n",
    "# Plot 2: Customer Income and Total Spending Correlation\n",
    "sns.scatterplot(x='TotalSpending', y='Income', data=dfe, color='#D1106F', ax=axs[0, 1])\n",
    "axs[0, 1].set_ylim(0, 122000000)\n",
    "axs[0, 1].set_xlim(0, 2700000)\n",
    "axs[0, 1].axvline(x=2540000, color='b', linestyle='--')\n",
    "axs[0, 1].set_title('Customer Income and Total Spending Correlation', fontsize=17, fontweight='bold', y=1.03)\n",
    "axs[0, 1].set_xlabel('Total Spending', fontsize=13.5)\n",
    "axs[0, 1].set_ylabel('Income', fontsize=13.5)\n",
    "axs[0, 1].grid(False)\n",
    "\n",
    "# Plot 3: Correlation Between Conversion Rate and Total Spending\n",
    "sns.scatterplot(x='TotalSpending', y='ConversionRate', data=dfe, color='#D1106F', ax=axs[1, 0])\n",
    "axs[1, 0].set_ylim(0, 3.8)\n",
    "axs[1, 0].set_title('Correlation Between Conversion Rate and Total Spending', fontsize=18, fontweight='bold', y=1.02)\n",
    "axs[1, 0].set_xlabel('Total Spending', fontsize=13.5)\n",
    "axs[1, 0].set_ylabel('Conversion Rate', fontsize=13.5)\n",
    "axs[1, 0].grid(False)\n",
    "\n",
    "# Plot 4: Correlation Between Conversion Rate and Age\n",
    "sns.scatterplot(x='Age', y='ConversionRate', data=dfe, color='#D1106F', ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Correlation Between Conversion Rate and Age', fontsize=18, fontweight='bold', y=1.02)\n",
    "axs[1, 1].set_xlabel('Age', fontsize=13.5)\n",
    "axs[1, 1].set_ylabel('Conversion Rate', fontsize=13.5)\n",
    "axs[1, 1].grid(False)\n",
    "\n",
    "# Adjust the layout to prevent overlapping of subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this multivariate analysis, several key relationships between variables were observed:\n",
    "\n",
    "1. **Income and Conversion Rate**:<br>There is a positive correlation between income and conversion rate. This suggests that as a customer's income increases, they are more likely to complete a purchase on our web platform after visiting. This could be due to higher disposable income allowing for more flexibility in purchasing decisions.\n",
    "\n",
    "2. **Total Spending and Income**:<br>There is a positive correlation between total spending and income. This indicates that customers with higher incomes tend to spend more. This could be a reflection of their greater purchasing power.\n",
    "\n",
    "3. **Total Spending and Conversion Rate**:<br> There is a positive correlation between total spending and conversion rate. This suggests that customers who spend more are also more likely to complete a purchase after visiting our web platform. This could be due to a higher level of engagement or interest in our products or services.\n",
    "\n",
    "4. **Conversion Rate and Age**:<br> There is no significant correlation between conversion rate and age. This indicates that the likelihood of a customer completing a purchase after visiting our web platform does not significantly vary with age. This could suggest that our platform appeals to a wide range of age groups.\n",
    "\n",
    "These insights can help us better understand the behavior of our customers and inform our marketing and sales strategies. However, it's important to remember that correlation does not imply causation, and further investigation may be needed to understand the underlying causes of these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x2 grid of subplots with a specific size and background color\n",
    "fig, axs = plt.subplots(2, 3, figsize=(24, 12), facecolor='#E8E8E8')\n",
    "\n",
    "# Define the color palette and order of age groups\n",
    "palette = ['#D1106F', '#00D19B', '#25A9D9']\n",
    "age_order = ['Young Adults', 'Middled-aged Adults', 'Old Adults']\n",
    "\n",
    "# Define a function to annotate the bars in a bar plot with their height values\n",
    "def annotate_barplot(barplot):\n",
    "    for p in barplot.patches:\n",
    "        height = p.get_height()\n",
    "        barplot.text(p.get_x() + p.get_width() / 2.,\n",
    "                     height + 0.01,\n",
    "                     '{:1.2f}'.format(height),\n",
    "                     ha=\"center\",\n",
    "                     fontweight='bold')\n",
    "\n",
    "# Create bar plots for different metrics by age group\n",
    "for i, metric in enumerate(['ConversionRate', 'TotalSpending', 'Income']):\n",
    "    barplot = sns.barplot(\n",
    "        data=dfe,\n",
    "        x='AgeGroup',\n",
    "        y=metric,\n",
    "        hue='AgeGroup',\n",
    "        order=age_order,\n",
    "        legend=False,\n",
    "        palette=palette,\n",
    "        errorbar=None,\n",
    "        edgecolor='black',\n",
    "        ax=axs[0, i]\n",
    "    )\n",
    "    annotate_barplot(barplot)\n",
    "    axs[0, i].set_title(f\"{metric} by Age Group\", fontsize=18, fontweight='bold', y=1.03)\n",
    "    axs[0, i].set_xlabel('Age Group', fontsize=12)\n",
    "    axs[0, i].set_ylabel(metric, fontsize=12)\n",
    "    axs[0, i].grid(False)\n",
    "\n",
    "# Create bar plots for Conversion Rate by Number of Children, Parental Status, and Education Level\n",
    "for i, metric in enumerate(['NumChild', 'Parent', 'Education']):\n",
    "    palette = ['#D1106F', '#00D19B', '#25A9D9', '#D16F11', '#6F11D1'][:len(dfe[metric].unique())]\n",
    "    barplot = sns.barplot(\n",
    "        x=metric,\n",
    "        y='ConversionRate',\n",
    "        hue=metric,\n",
    "        data=dfe,\n",
    "        legend=False,\n",
    "        palette=palette,\n",
    "        errorbar=None,\n",
    "        edgecolor='black',\n",
    "        ax=axs[1, i]\n",
    "    )\n",
    "    annotate_barplot(barplot)\n",
    "    axs[1, i].set_title(f\"Conversion Rate by {metric}\", fontsize=18, fontweight='bold', y=1.03)\n",
    "    axs[1, i].set_xlabel(metric, fontsize=12)\n",
    "    axs[1, i].set_ylabel('Conversion Rate', fontsize=12)\n",
    "    axs[1, i].grid(False)\n",
    "\n",
    "# Adjust the layout to prevent overlapping of subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my analysis of customer behavior, i've identified several key trends:\n",
    "- The highest conversion rate is for customers in the Old Adults age group (> 59 years), and based on total spending, this Old Adults age group has the most significant spending, reaching more than 700,000. This can indicate high trust and engagement in the online shopping experience. They may be more likely to complete a purchase because they understand and feel comfortable with the process.\n",
    "- Customers who do not have children or are not parents have a higher conversion rate than customers who already have children. However, the distribution of our customers in the previous pie chart shows that most of our customers already have children or are married. Therefore, the potential for a higher conversion rate exists for customers who do not have children.\n",
    "- Customers who don't have a degree (still in high school) have the lowest conversion rate compared to customers who already have a degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of numerical features for which we want to compute correlations\n",
    "numerical_features = [\n",
    "       'Income', 'Recency', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', \n",
    "       'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', \n",
    "       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Complain',\n",
    "       'Response', 'Age', 'NumChild', 'TotalAcceptedCmp', 'TotalSpending', 'TotalTrx', \n",
    "       'ConversionRate'\n",
    "]\n",
    "\n",
    "# Create a new figure with a specific size and background color\n",
    "plt.figure(figsize=(16, 9.5), facecolor='#E8E8E8')\n",
    "\n",
    "# Compute the correlation matrix for the numerical features and plot it as a heatmap\n",
    "correlation_matrix = dfe[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "\n",
    "# Set the title of the heatmap\n",
    "plt.title('Correlation Heatmap', fontsize=18, fontweight='bold', y=1.02)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the correlation Heatamp above, we can see that the highest correlation is between Income and Conversion Rate. This indicates that the higher the income, the higher the conversion rate. This can be seen from the scatter plot above, where the higher the income, the higher the conversion rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Data Cleaning & Preprocessing\n",
    "Goals : Preparing raw data into clean data ready to be processed by machine learning<br><br>\n",
    "Objective : \n",
    "- Handle Missing Values\n",
    "- Handle Duplicate Values\n",
    "- Handle Infinity values \n",
    "- Feature Selection \n",
    "- Feature Encoding\n",
    "- Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of previous dataframe for next step (Data Preprocessing)\n",
    "dfp = dfe.copy()\n",
    "\n",
    "# Print missing values\n",
    "missing_col = dfp.isna().sum()\n",
    "display_missing_col = missing_col[missing_col > 0]\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "missing_percentage = (dfp.isna().sum() / len(dfp)) * 100\n",
    "display_missing_percentage = missing_percentage[missing_percentage > 0]\n",
    "\n",
    "# Format the percentages\n",
    "display_missing_percentage = display_missing_percentage.map('{:.2f}%'.format)\n",
    "\n",
    "print(f'Missing Values : \\n \\n{display_missing_col}')\n",
    "print(f'\\nPercentage of Missing Values : \\n \\n{display_missing_percentage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the percentage of missing values in two features, `Income` and `ConversionRate`, we can see that the percentage of missing values in the `Income` is not too large and the distribution of the data is positively skewed, so we can fill in the missing values with the median value. However, the percentage of missing values in the `ConversionRate` seems reasonable to drop the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns 'NumWebPurchases', 'NumWebVisitsMonth', and 'ConversionRate' from the dataframe 'dfp'\n",
    "missing_cr = dfp[['NumWebPurchases', 'NumWebVisitsMonth', 'ConversionRate']]\n",
    "\n",
    "# Filter the rows in 'missing_cr' where any of the columns have missing values\n",
    "missing_crdf = missing_cr[missing_cr.isna().any(axis=1)]\n",
    "\n",
    "# Print the dataframe 'missing_crdf' which contains the rows with missing values\n",
    "print(f\"Highlighted Missing values : \\n\")\n",
    "display(missing_crdf)\n",
    "\n",
    "# This statement indicates that the missing values in the 'ConversionRate' column are not missing at random. \n",
    "# This could mean that there's a specific reason or pattern behind the missing values in this column.\n",
    "print('*Conversion Rate not missing at Random*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason to drop this missing value at conversion rate is because My Conversion Rate formula is based on the number of visitors who complete a purchase after visiting our web platform. So, if the value of the conversion rate is missing, it means that the customer has not made a purchase after visiting our web platform. So, we can drop the missing values in the `ConversionRate` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total null on income and conversion rate\n",
    "total_null_income = dfp['Income'].isna().sum()\n",
    "total_null_conrate = dfp['ConversionRate'].isna().sum()\n",
    "print(f\"Total Missing Values on Income Column = {total_null_income}\")\n",
    "print(f\"Total Missing Values on Conversion Rate Column = {total_null_conrate}\")\n",
    "\n",
    "# print median income\n",
    "median_income = dfp['Income'].median()\n",
    "print(f\"\\nIncome Median to fill the missing value: {median_income}\")\n",
    "\n",
    "# handle missing values with fill and drop method\n",
    "dfp['Income'].fillna(dfp['Income'].median(), inplace=True)\n",
    "dfp.dropna(subset=['ConversionRate'], inplace=True)\n",
    "\n",
    "# checking missing values if still exist\n",
    "nonull_income = dfp['Income'].isna().sum()\n",
    "nonull_conrate = dfp['ConversionRate'].isna().sum()\n",
    "print(f\"\\nMissing Values on Income Column after handling = {nonull_income}\")\n",
    "print(f\"Missing Values on Conversion Rate Column after handling = {nonull_conrate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miising Values Handled respectively in `Income` and `ConversionRate` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of duplicate rows in the dataframe 'dfp'\n",
    "total_duplicate = dfp.duplicated().sum()\n",
    "\n",
    "# Print the total number of duplicate rows\n",
    "print(f\"Total Duplicated Data = {total_duplicate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the Infinity Value On Conversion Rate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print count Infiinity values in dataframe\n",
    "count_inf = dfp.map(lambda x: isinstance(x, float) and x == float('inf')).sum().sum()\n",
    "print(f\"Count of Infinity Values :\\nIt Contains {str(count_inf)} Infinite values in dataframe\")\n",
    "\n",
    "# print column where infinity values exist\n",
    "col_inf = dfp.columns[dfp.map(lambda x: isinstance(x, float) and x == float('inf')).any()]\n",
    "print(\"\\nColumns where Infinity values exist:\")\n",
    "print(\", \".join(col_inf))\n",
    "\n",
    "# Replace infinity values with NaN and drop them\n",
    "dfp.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "dfp.dropna(inplace=True)\n",
    "\n",
    "# Print the number of entries in the dataframe\n",
    "print(f\"\\nEntries after cleaning: {len(dfp)}\")\n",
    "\n",
    "# Check if there are still any infinity values\n",
    "no_inf = dfp.map(lambda x: isinstance(x, float) and x == float('inf')).sum().sum()\n",
    "print(f\"Infinity values remaining: {no_inf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This infinity value is caused by the number of visitors who have not made a purchase after visiting our web platform. So, we can replace the infinity value with 0 and drop the missing values in the `ConversionRate` feature. if we don't handle this infinity value, the data cannot be standardized and will cause an error in the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinity values with NaN and drop them\n",
    "dfp.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "dfp.dropna(inplace=True)\n",
    "\n",
    "# Print the number of entries in the dataframe\n",
    "print(f\"Entries after cleaning: {len(dfp)}\")\n",
    "\n",
    "# Check if there are still any infinity values\n",
    "no_inf = dfp.map(lambda x: isinstance(x, float) and x == float('inf')).sum().sum()\n",
    "print(f\"Infinity values remaining: {no_inf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- infinity Values handled in `ConversionRate` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Outliers\n",
    "- The outlier handling method that I use is capping the outliers to the lower/upper bound. This approach involves replacing the extreme values with a specified minimum and maximum value. It is a suitable method when we don't want to lose data, but at the same time, we want to limit the effect of the extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to cap outliers in the data\n",
    "def cap_outliers(data, columns):\n",
    "    # Create a copy of the data to avoid modifying the original dataframe\n",
    "    result = data.copy()\n",
    "    # Loop over each column specified\n",
    "    for col in columns:\n",
    "        # Calculate the first quartile (Q1)\n",
    "        Q1 = result[col].quantile(0.25)\n",
    "        # Calculate the third quartile (Q3)\n",
    "        Q3 = result[col].quantile(0.75)\n",
    "        # Calculate the Interquartile Range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # Define the lower bound as Q1 - 1.5 * IQR\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        # Define the upper bound as Q3 + 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Replace values below the lower bound with the lower bound\n",
    "        result[col] = np.where(result[col] < lower_bound, lower_bound, result[col])\n",
    "        # Replace values above the upper bound with the upper bound\n",
    "        result[col] = np.where(result[col] > upper_bound, upper_bound, result[col])\n",
    "    # Return the dataframe with capped outliers\n",
    "    return result\n",
    "\n",
    "# Define the columns to cap outliers in\n",
    "outliers = ['Income', 'MntMeatProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n",
    "            'NumWebPurchases', 'NumCatalogPurchases', 'NumWebVisitsMonth', 'Age', 'TotalTrx', 'ConversionRate'] \n",
    "\n",
    "# Apply the cap_outliers function to the dataframe 'dfp' and the specified columns\n",
    "dfp_noutlier = cap_outliers(dfp, outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As i said before, the `Z_CostContact` and `Z_Revenue` features only contain a single unique value. These features do not provide any variability or valuable information for our analysis or predictive modeling, and will be dropped in this section.\n",
    "- The `Unnamed: 0` feature appears to be an index column. Since Pandas DataFrames automatically provide an index, this column is redundant and will be dropped in this section.\n",
    "- The `Dt_Customer` feature is a date column. This feature will be dropped in this section because it is not needed in the machine learning model.\n",
    "- The `ID` feature is a unique identifier for each customer. This feature will be dropped in this section because it is not needed in the machine learning model.\n",
    "- The `Year_Birth` feature is a date column. This feature will be dropped in this section because it is not needed in the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns from the dataframe 'dfp_noutlier'\n",
    "dfp_noutlier = dfp_noutlier.drop(columns=['Unnamed: 0', 'ID', 'Year_Birth', 'Dt_Customer', 'Z_CostContact', 'Z_Revenue'])\n",
    "dfp_noutlier.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Encoding\n",
    "Features to label Encode :<br>\n",
    "- Education\n",
    "- Age Group\n",
    "because the features have an order of values(ordinal data)\n",
    "\n",
    "Features to One Hot Encode: <br>\n",
    "- Marital_Status\n",
    "- Parent\n",
    "because the features don't have an order of values(nominal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encding\n",
    "# Initialize Label Encoder as le\n",
    "le = LabelEncoder()\n",
    "\n",
    "dfp_noutlier['Education'] = le.fit_transform(dfp_noutlier['Education'])\n",
    "dfp_noutlier['AgeGroup'] = le.fit_transform(dfp_noutlier['AgeGroup'])\n",
    "\n",
    "\n",
    "# One hot Encoding\n",
    "ms_encoded = pd.get_dummies(dfp_noutlier['Marital_Status'], prefix='Status').astype(int)\n",
    "dfp_noutlier = pd.concat([dfp_noutlier, ms_encoded], axis=1)\n",
    "\n",
    "parent_encoded = pd.get_dummies(dfp_noutlier['Parent'], prefix='Parent').astype(int)\n",
    "dfp_noutlier = pd.concat([dfp_noutlier, parent_encoded], axis=1)\n",
    "\n",
    "# drop marital status and parent column after encoded(redundant)\n",
    "dfp_noutlier.drop(columns=['Marital_Status', 'Parent'], inplace=True)\n",
    "\n",
    "print('\\ndataframe after feature encoding :')\n",
    "display(dfp_noutlier.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standarization is needed to make the data have a mean of 0 and a standard deviation of 1. This is useful for Unsupervised Machine Learning models such as KMeans Clustering. Because the KMeans Clustering model uses the Euclidean Distance method to calculate the distance between data points, the data must be standardized so that the distance between data points is not too far apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inititalize standard scaler as scaler\n",
    "scaler = StandardScaler()\n",
    "# Standardize the data\n",
    "scaled_data = scaler.fit_transform(dfp_noutlier)\n",
    "\n",
    "# new dataframe with scaled data\n",
    "scaled_dfp = pd.DataFrame(scaled_data, columns=dfp_noutlier.columns, index=dfp_noutlier.index)\n",
    "\n",
    "print('\\ndataframe after scaled(standarized) :')\n",
    "scaled_dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scaled_dfp.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 : Modelling\n",
    "Goals : Group customers into several clusters<br><br>\n",
    "Objective : \n",
    "Apply the k-means clustering algorithm to the existing dataset, choose the correct number of clusters by looking at the elbow method, and evaluate using the silhouette score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is needed to reduce the dimensionality of the data. Because the data has 39 features, it is necessary to reduce the dimensionality of the data so that the data is not too complex and the machine learning model can run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a PCA object with 2 components\n",
    "# PCA (Principal Component Analysis) is a technique used to reduce the dimensionality of the data\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "# Fit the PCA model to the scaled data and transform the data into the first two principal components\n",
    "# The transformed data is then converted into a DataFrame with the same index as 'dfp_noutlier'\n",
    "dfpca = pd.DataFrame(pca.fit_transform(scaled_dfp), index=dfp_noutlier.index)\n",
    "\n",
    "# Rename the columns of the DataFrame to 'PC1' and 'PC2' for better readability\n",
    "# 'PC1' and 'PC2' represent the first and second principal components respectively\n",
    "dfpca.rename(columns={0:'PC1', 1:'PC2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal n cluster with Elbow Method and Silhouette Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow Method is used to find the optimal number of clusters by looking at the elbow point in the graph. The elbow point is the point where the graph starts to flatten.<br>\n",
    "And Silhouette Method is used to find the optimal number of clusters by looking at the highest silhouette score because the silhouette score is a metric used to calculate the distance between clusters. The higher the silhouette score, the better the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store inertia and silhouette scores for different numbers of clusters\n",
    "inertia = []\n",
    "silhouette = []\n",
    "\n",
    "# Loop over a range of numbers from 2 to 9 (inclusive) to represent different numbers of clusters\n",
    "for k in range(2, 10):\n",
    "    # Initialize a KMeans object with 'k' clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=248, n_init=\"auto\")\n",
    "    # Fit the KMeans model to the PCA-transformed data\n",
    "    kmeans.fit(dfpca)\n",
    "    # Append the inertia of the model to the 'inertia' list\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    # Get the cluster labels predicted by the model\n",
    "    cluster_label = kmeans.labels_\n",
    "    # Calculate the silhouette score of the model and append it to the 'silhouette' list\n",
    "    silhouette.append(silhouette_score(dfpca, cluster_label))\n",
    "\n",
    "# Create a new figure and axis for plotting\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_facecolor(\"#E8E8E8\")\n",
    "\n",
    "# Set the labels for the x-axis and the left y-axis\n",
    "ax1.set_xlabel(\"k\")\n",
    "ax1.set_ylabel(\"inertia score\", color=\"tab:blue\")\n",
    "# Plot the inertia scores against the number of clusters\n",
    "ax1.plot(range(2, 10), inertia, marker=\"o\", linestyle=\"--\", color=\"tab:blue\", label=\"inertia\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "# Create a second y-axis for the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "# Set the label for the right y-axis\n",
    "ax2.set_ylabel(\"silhouette score\", color=\"tab:red\")\n",
    "# Plot the silhouette scores against the number of clusters\n",
    "ax2.plot(range(2, 10), silhouette, marker=\"o\", linestyle=\"--\", color=\"tab:red\", label=\"silhouette\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "# Combine the legends for the two plots\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc=\"upper right\")\n",
    "\n",
    "# Set the title for the plot and display the plot\n",
    "plt.title(\"Inertia-Silhouette Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Elbow Method and Silhouette Method, the optimal number of clusters is 3 clusters, but I choose 4 clusters because the customer segmentation will be more detailed, and the silhouette score is still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 subplot with a specified figure size\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 8))\n",
    "# Set the background color of the figure\n",
    "fig.set_facecolor(\"#E8E8E8\")\n",
    "\n",
    "# Loop over a range of numbers from 2 to 5 (inclusive) to represent different numbers of clusters\n",
    "for i in range(2, 6):\n",
    "    # Initialize a KMeans object with 'i' clusters\n",
    "    kmeans = KMeans(n_clusters=i, random_state=248, n_init='auto')\n",
    "    # Calculate the row (q) and column (mod) indices for the subplot\n",
    "    q, mod = divmod(i, 2)\n",
    "    # Initialize a SilhouetteVisualizer with the KMeans model and specify the colors to use\n",
    "    visualizer = SilhouetteVisualizer(kmeans, colors=\"yellowbrick\", ax=ax[q - 1][mod])\n",
    "    # Fit the SilhouetteVisualizer to the PCA-transformed data\n",
    "    visualizer.fit(dfpca)\n",
    "    # Set the title for the subplot\n",
    "    ax[q - 1][mod].set_title(f'Silhouette plot for {i} clusters', fontsize=12, fontweight='bold')\n",
    "    # Set the x-label for the subplot\n",
    "    ax[q - 1][mod].set_xlabel('Silhouette Coefficient Values')\n",
    "    # Set the y-label for the subplot\n",
    "    ax[q - 1][mod].set_ylabel('Cluster Label')\n",
    "# Adjust the layout of the subplots to ensure that the plots do not overlap\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the second silhouette score analysis plot, it is evident that the data points within each of the four clusters are relatively close to each other, indicating a good degree of cohesion within each cluster. Furthermore, the average silhouette score remains consistent across the clusters, suggesting a balanced distribution of data points among them. This balance is a positive sign, as it indicates that no single cluster is overly dominant, which could potentially skew the overall analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit KMeans Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will fit the KMeans model with the optimal number of clusters, which is 4 clusters. The KMeans model will be fitted with the PCA data. and the result of the KMeans model will be saved in the `cluster` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimal number of clusters to 4\n",
    "k_optimal = 4\n",
    "\n",
    "# Initialize a KMeans object with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=10000, n_init='auto')\n",
    "\n",
    "# Fit the KMeans model to the PCA-transformed data\n",
    "kmeans.fit(dfpca)\n",
    "\n",
    "# Add the cluster labels predicted by the model to the dataframe 'dfpca'\n",
    "dfpca['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Display the dataframe 'dfpca'\n",
    "dfpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure with a specified size and background color\n",
    "plt.figure(figsize=(12,8), facecolor='#E8E8E8')\n",
    "\n",
    "# Create a scatter plot of the first principal component ('PC1') against the second principal component ('PC2')\n",
    "# The points are colored according to their cluster label\n",
    "palt = ['#D1106F', '#00D19B', '#25A9D9', '#D16F11']\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=dfpca, palette=palt)\n",
    "\n",
    "# Get the coordinates of the cluster centers from the KMeans model\n",
    "centroids = kmeans.cluster_centers_\n",
    "# Plot the cluster centers as black 'x' markers\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, alpha=0.8, marker='x')\n",
    "\n",
    "# Set the title for the plot\n",
    "plt.title('K-Means Clustering', fontsize=18, fontweight='bold', y=1.03)\n",
    "# Set the label for the x-axis\n",
    "plt.xlabel('PCA 1', fontsize=12)\n",
    "# Set the label for the y-axis\n",
    "plt.ylabel('PCA 2', fontsize=12)\n",
    "# Remove the grid from the plot\n",
    "plt.grid(False)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the KMeans model, the data points are divided into 4 clusters, namely cluster 0, cluster 1, cluster 2, and cluster 3. The data points in each cluster are not too far apart, so the KMeans model is good enough to cluster the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Customer Personality Analysis For Marketing Retargeting\n",
    "**Goal** : Develop a targeted marketing strategy to optimize costs and boost revenue.<br><br>\n",
    "**Objective** : \n",
    "- Interpretate the clusters\n",
    "- Use visualizations to identify key characteristics and behaviors of each customer group. This will help us understand our customers better and tailor our marketing strategies to their specific needs and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe 'dfp_noutlier' and store it in 'df_clust'\n",
    "df_clust = dfp_noutlier.copy()\n",
    "\n",
    "# Get the 'cluster' column from the dataframe 'dfpca'\n",
    "label = dfpca['Cluster']\n",
    "\n",
    "# Add the 'cluster' column to the dataframe 'df_clust'\n",
    "df_clust['Cluster'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of metrics for creating box plots\n",
    "metrics = ['Income', 'TotalSpending', 'ConversionRate', 'Loyalty', 'TotalTrx', 'Recency', 'Cluster']\n",
    "# Remove 'cluster' from the list as it is not a metric\n",
    "metrics.remove('Cluster')\n",
    "\n",
    "# Calculate the number of rows needed for the subplots\n",
    "n = len(metrics)\n",
    "ncols = 3\n",
    "nrows = n // ncols if n % ncols == 0 else n // ncols + 1\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(24, nrows*5))\n",
    "fig.set_facecolor('#E8E8E8')  # Set the figure background color\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Define the order of clusters for display in the box plots\n",
    "cluster_order = [1, 2, 3, 0]\n",
    "\n",
    "# Define the color palette\n",
    "palt = ['#D1106F', '#00D19B', '#25A9D9', '#D16F11']\n",
    "\n",
    "# Create subplots for each feature\n",
    "for i, feature in enumerate(metrics):\n",
    "    # Create a box plot for the current feature\n",
    "    sns.boxplot(data=df_clust, y=feature, x='Cluster', hue='Cluster', palette=palt, ax=ax[i], order=cluster_order, hue_order=cluster_order)\n",
    "    ax[i].set_title(feature)  # Set the title of the subplot\n",
    "    ax[i].grid(False)  # Remove the grid from the subplot\n",
    "    ax[i].legend(loc='center left', bbox_to_anchor=(1, 0.8))  # Move the legend to the outside of the subplot\n",
    "    # Change the labels of the hue legend to more descriptive labels\n",
    "    hue_labels = ['High Spender', 'Mid Spender', 'Low Spender', 'Risk Churn']\n",
    "    legend = ax[i].get_legend()\n",
    "    for text, label in zip(legend.texts, hue_labels):\n",
    "        text.set_text(label)\n",
    "\n",
    "# Adjust the layout of the subplots to prevent overlapping display the figure\n",
    "plt.tight_layout()  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe 'df_clust' and store it in 'interpretation'\n",
    "interpretation = df_clust.copy()\n",
    "\n",
    "# Rename cluster value to more descriptive labels\n",
    "interpretation['Cluster'] = interpretation['Cluster'].map({\n",
    "    1: 'High Spender', \n",
    "    2: 'Mid Spender', \n",
    "    3: 'Low Spender', \n",
    "    0: 'Risk Churn'\n",
    "})\n",
    "\n",
    "# Rename the columns to more descriptive labels\n",
    "rename_cols = {\n",
    "    'MntCoke': 'CokeProducts',\n",
    "    'MntFruits': 'FruitsProducts',\n",
    "    'MntMeatProducts': 'MeatProducts',\n",
    "    'MntFishProducts': 'FishProducts',\n",
    "    'MntSweetProducts': 'SweetProducts',\n",
    "    'MntGoldProds': 'GoldProducts'\n",
    "}\n",
    "interpretation.rename(columns=rename_cols, inplace=True)\n",
    "# Define the columns to be used in the groupby operation\n",
    "intr_metrics = [\n",
    "    'ConversionRate',\n",
    "    'TotalSpending', \n",
    "    'Income',\n",
    "    'TotalTrx',\n",
    "    'TotalAcceptedCmp',\n",
    "    'CokeProducts', \n",
    "    'FruitsProducts', \n",
    "    'MeatProducts', \n",
    "    'FishProducts', \n",
    "    'SweetProducts', \n",
    "    'GoldProducts'\n",
    "]\n",
    "# Set the float format to display numbers with a thousands separator\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Calculate and display the total sum of each metric for each cluster\n",
    "sum_interpretation = interpretation.groupby('Cluster')[intr_metrics].sum().sort_values('TotalSpending', ascending=False).astype(float).reset_index()\n",
    "print('Total/SUM Clusters Metrics :\\n')\n",
    "display(sum_interpretation)\n",
    "\n",
    "# Calculate and display the average value of each metric for each cluster\n",
    "average_interpretation = interpretation.groupby('Cluster')[intr_metrics].mean().sort_values('TotalSpending', ascending=False).astype(float).reset_index()\n",
    "print('\\n\\nAverage/Mean Clusters Metrics :\\n')\n",
    "display(average_interpretation)\n",
    "\n",
    "# Calculate and display the maximum value of each metric for each cluster\n",
    "max_interpretation = interpretation.groupby('Cluster')[intr_metrics].max().sort_values('TotalSpending', ascending=False).astype(float).reset_index()\n",
    "print('\\n\\nMax Clusters Metrics :\\n')\n",
    "display(max_interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(30, 15))\n",
    "fig.set_facecolor('#E8E8E8')\n",
    "\n",
    "palt = ['#D1106F', '#00D19B', '#25A9D9', '#D16F11']\n",
    "orderseg = ['High Spender', 'Mid Spender', 'Low Spender', 'Risk Churn']\n",
    "\n",
    "# loop over each feature and create a bar plot on a separate subplot\n",
    "for feature, ax in zip(intr_metrics, ax.flatten()):\n",
    "    # Create a bar plot for the current feature\n",
    "    barplot = sns.barplot(data=sum_interpretation, x='Cluster', y=feature, ax=ax, palette=palt, hue='Cluster', order=orderseg, hue_order=orderseg, errorbar=None, edgecolor='black')\n",
    "    ax.set_title(f'SUM {feature} by Cluster', fontsize=15, fontweight='bold', pad=5)\n",
    "    ax.set_xlabel('Cluster', fontsize=12)\n",
    "    ax.set_ylabel(feature, fontsize=12)\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Add number annotations inside the bars\n",
    "    for p in barplot.patches:\n",
    "        barplot.annotate(format(p.get_height(), '.2f'), \n",
    "                         (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                         ha = 'center', va = 'center', \n",
    "                         xytext = (0, 12), \n",
    "                         textcoords = 'offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(30, 15))\n",
    "fig.set_facecolor('#E8E8E8')\n",
    "\n",
    "palt = ['#D1106F', '#00D19B', '#25A9D9', '#D16F11']\n",
    "orderseg = ['High Spender', 'Mid Spender', 'Low Spender', 'Risk Churn']\n",
    "\n",
    "# loop over each feature and create a bar plot on a separate subplot\n",
    "for feature, ax in zip(intr_metrics, ax.flatten()):\n",
    "    # Create a bar plot for the current feature\n",
    "    barplot = sns.barplot(data=average_interpretation, x='Cluster', y=feature, ax=ax, palette=palt, hue='Cluster', order=orderseg, hue_order=orderseg, errorbar=None, edgecolor='black')\n",
    "    ax.set_title(f'AVERAGE {feature} by Cluster', fontsize=15, fontweight='bold', pad=10)\n",
    "    ax.set_xlabel('Cluster', fontsize=12)\n",
    "    ax.set_ylabel(feature, fontsize=12)\n",
    "    ax.grid(False)\n",
    "\n",
    "    # Add number annotations inside the bars\n",
    "    for p in barplot.patches:\n",
    "        barplot.annotate(format(p.get_height(), '.2f'), \n",
    "                         (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                         ha = 'center', va = 'center', \n",
    "                         xytext = (0, 12), \n",
    "                         textcoords = 'offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Interpretation and Marketing Recommendations:\n",
    "\n",
    "#### 1. **High Spender:**\n",
    "   - **Cluster Characteristics:**\n",
    "     - Highest Sum Total Spending: $702,330,000.00\n",
    "     - Highest Average Conversion Rate: 2.14\t\n",
    "     - Highest Average Income: $75,027,988.30\n",
    "     - Average Total Transaction : 20.15\n",
    "     - Top Spending Categories: Coke and Meat Products\n",
    "\n",
    "   - **Interpretation:**\n",
    "     - This cluster comprises customers who exhibit an exceptional conversion rate, spending the most on the platform.\n",
    "     - Their high income and infrequent visits indicate that when they engage, they make substantial purchases.\n",
    "     - The primary spending focus is on Coke and meat products.\n",
    "\n",
    "   - **Marketing Recommendation:**\n",
    "     - Implement personalized retargeting campaigns based on previous purchases, emphasizing exclusive offers on Coke and meat products.\n",
    "     - Utilize high-income targeting for promotions and loyalty programs to further increase customer spend.\n",
    "\n",
    "#### 2. **Mid Spender:**\n",
    "   - **Cluster Characteristics:**\n",
    "     - Sum Total Spending: $425,130,000.00 \n",
    "     - Average Conversion Rate: 1.43\n",
    "     - Average Income: $64,701,803.78\n",
    "     - Average Total Transaction 23.47\n",
    "     - Top Spending Categories: Coke and Meat Products\n",
    "\n",
    "   - **Interpretation:**\n",
    "     - This group represents customers with a good conversion rate, showing a moderate level of spending and income.\n",
    "     - The Average Total Transaction this cluster has is higher than the high spender.   \n",
    "     - Similar to the High Spender cluster, Coke and meat products are significant areas of expenditure.\n",
    "\n",
    "   - **Marketing Recommendation:**\n",
    "     - Implement retargeting strategies to promote a broader range of products to increase the average transaction value.\n",
    "     - Consider introducing loyalty programs to encourage more frequent visits and higher spending.\n",
    "\n",
    "#### 3. **Low Spender:**\n",
    "   - **Cluster Characteristics:**\n",
    "     - Sum Total Spending: $151,481,000.00\n",
    "     - Average Conversion Rate: 0.81\n",
    "     - Average Income: $49,095,142.69\n",
    "     - Average Total Transaction 15.95\n",
    "     - Top Spending Categories: Coke and Meat Products\n",
    "\n",
    "   - **Interpretation:**\n",
    "     - This cluster represents customers with lower conversion rates and spending levels compared to the previous clusters.\n",
    "     - While income is relatively high, the spending behavior suggests potential for increased engagement.\n",
    "\n",
    "   - **Marketing Recommendation:**\n",
    "     - Implement targeted promotions for a wider range of products to encourage increased spending.\n",
    "     - Launch special offers and discounts to attract this segment and increase their frequency of visits.\n",
    "\n",
    "#### 4. **Risk Churn:**\n",
    "   - **Cluster Characteristics:**\n",
    "     - Lowest Sum Total Spending: $66,706,000.00 \n",
    "     - Lowest Average Conversion Rate: 0.30\n",
    "     - Average Income: $32,985,988.58\n",
    "     - Average Total Transaction 7.04\n",
    "     - Top Spending Categories: Coke and Meat Products\n",
    "\n",
    "   - **Interpretation:**\n",
    "     - This cluster represents customers with a relatively low conversion rate, lower spending, and potential risk of churn.\n",
    "     - The lowest average conversion rate means this cluster often visits our online web platform but doesn't finish the transaction.\n",
    "     - Similar spending patterns in Coke and meat products, but the lower conversion rate indicates a need for targeted retention efforts.\n",
    "\n",
    "   - **Marketing Recommendation:**\n",
    "     - Implement aggressive retargeting campaigns with personalized incentives to prevent churn.\n",
    "     - Focus on customer satisfaction initiatives and exclusive offers to re-engage and retain customers.\n",
    "\n",
    "### General Recommendations:\n",
    "- **Cross-Sell Strategies:**\n",
    "  - Leverage data to identify cross-sell opportunities within each cluster, encouraging customers to explore additional product categories.\n",
    "\n",
    "- **Dynamic Pricing:**\n",
    "  - Implement dynamic pricing strategies based on customer behavior and purchase history to optimize revenue from each cluster.\n",
    "\n",
    "- **Customer Segmentation Refinement:**\n",
    "  - Regularly review and refine customer segmentation to adapt to changing market dynamics and customer preferences.\n",
    "\n",
    "- **Invest in Data Analytics:**\n",
    "  - Continue investing in advanced analytics to uncover deeper insights and refine marketing strategies based on evolving customer behaviors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
